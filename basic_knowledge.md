# LLM应用开发关键知识点总结

## 1. Decoder-only vs Encoder-only vs Encoder-Decoder

### 核心区别与适用场景

- **Decoder-only**（代表模型：GPT系列、LLaMA、Mistral、Qwen、DeepSeek等主流开源/闭源模型）
  - 特点：自回归（autoregressive）生成，单向从左到右依次预测下一个token
  - 优势：生成能力极强，适合开放式、交互式任务
  - 典型应用：聊天机器人、文本续写、代码生成、Agent工具调用、建议生成、创意写作
  - 当前趋势：2025-2026年95%+的生成类应用（包括RAG、Agent）都使用Decoder-only架构
  - 适用阶段：几乎所有以“生成”为核心的LLM应用

- **Encoder-only**（代表模型：BERT、RoBERTa、DeBERTa等）
  - 特点：双向编码，整句上下文同时理解
  - 优势：对上下文的理解精度最高，适合“理解”而非“生成”任务
  - 典型应用：文本分类、情感分析、命名实体识别（NER）、语义相似度、embedding提取（RAG检索阶段常用）
  - 适用阶段：需要高质量表示或分类的子模块（常作为辅助模型）

- **Encoder-Decoder**（代表模型：T5、Flan-T5、BART、mT5）
  - 特点：编码器双向理解输入，解码器自回归生成输出，经典seq2seq架构
  - 优势：结构化转换任务表现稳定
  - 典型应用：机器翻译、文本摘要、文本改写、问答（输入-输出明确对应）
  - 适用阶段：输入输出有明确序列转换关系的任务

### 应用开发中的选择建议
- **生成主导的应用**（聊天、Agent、建议、代码、创作） → 优先Decoder-only（几乎是默认选择）
- **纯理解/分类/embedding** → Encoder-only
- **严格的序列到序列转换** → Encoder-Decoder（或用Decoder-only + 强prompt模拟）
- **错误选择后果**：选错架构会导致RAG/Agent效果明显下降、生成不稳定或效率低下

## 2. Self-Attention 的核心代价：O(n²) 时间 & 空间复杂度

### 核心概念
- n = 序列长度（context length，即输入 + 历史对话 + 检索内容等的总token数）
- Self-Attention机制需要对序列中每个token与其他所有token计算注意力 → 时间和空间复杂度均为 **O(n²)**
- 影响：
  - 上下文越长，推理速度越慢
  - 显存/内存占用呈平方级增长
  - 云API按token计费时，成本随n快速上升（接近指数级）

### 实际应用中的表现
- 长上下文（128k、200k、1M、2M）非常贵、慢、吃资源
- 典型问题：
  - 推理延迟显著增加
  - 云API单次查询成本大幅上升
  - 本地部署时显存容易溢出

### 开发中的应对策略（API调用场景为主）

#### 早期阶段（需求分析 & 模型选型）
- 估算典型和最坏情况下的上下文长度（n）
- 优先选择支持长上下文且性价比高的API（如Grok、Gemini、Claude的长版本）
- 关注是否支持prompt caching（重复内容缓存可降成本90%+）
- 粗算成本模型：成本 ≈ (输入tokens × 单价 + 输出tokens × 单价) × 频次

#### 中期阶段（原型 & 优化）
- 主动压缩上下文：
  - RAG只注入top-k相关chunk
  - 历史对话用摘要代替全量
  - 工具返回的长数据先总结/提取关键指标
- 测试不同n下的延迟和成本，设定目标（如平均<5k-8k tokens，延迟<3-5s）
- 使用多模型路由：简单查询用短上下文便宜模型，复杂长分析用长上下文模型

#### 后期阶段（上线 & 运维）
- 实时token限流（超阈值提示用户精简或新会话）
- 监控长上下文查询占比，持续优化
- 利用厂商的context caching机制
- 定期复盘API价格与性能，选择最优提供商

### 总结一句话
**Decoder-only 是当下生成类应用的绝对主流，但Self-Attention的O(n²)代价决定了“控制有效上下文长度”是API调用开发中最重要的工程实践之一。**

# KV Cache（Key-Value Cache）机制 - 通用 LLM API 开发应用指南

## 核心概念
- KV Cache 是自回归（autoregressive）生成加速的核心机制
- 生成时：只计算**新增 token 的 Query**，与之前所有已缓存的 **Key / Value** 做注意力计算
- 好处：避免每次从头计算整个序列的 O(n²)，实现近似 O(n) 的增量推理
- 代价：在多轮对话、长 Agent 轨迹、长上下文场景下，KV Cache 会持续累积，导致：
  - 内存占用线性增长（每 token 缓存 K 和 V 向量）
  - 上下文窗口很快被占满（触发“context too long”错误）
  - API 延迟增加、成本上升（因为总 tokens 包含历史）

## 为什么对 API 调用开发者重要？
- 你无法直接操作 KV Cache（由 OpenAI、Grok、Gemini、Claude 等提供商管理）
- 但它直接决定：
  - 实际可用上下文长度（即使标称 128k/1M，也可能因 KV 膨胀提前失效）
  - 多轮对话的稳定性和成本
  - 长轨迹 Agent 的可行性

## 开发阶段的应用方式

### 早期阶段：需求分析 & 模型选型
- 评估应用是否涉及**多轮交互**或**长轨迹**（聊天、Agent、连续决策等）
- 优先选择支持 **GQA（Group Query Attention）** 或 **MQA（Multi-Query Attention）** 的模型 API：
  - 显著减小 KV Cache 大小（节省 50-75% 内存）
  - 常见代表：Grok 系列、LLaMA-3 系、Mistral 系、Gemini 最新版、Claude 部分版本
- 查看 API 文档中的：
  - 上下文窗口大小
  - 是否支持 prompt/context caching（复用 KV 缓存，成本大幅下降）
- 规划“状态管理”策略：是无状态（每轮独立）还是有状态（带历史）

### 中期阶段：原型构建 & 上下文管理优化
- 主动压缩上下文，减少 KV 累积：
  - 用摘要替换历史对话（“基于前文总结：……”）
  - RAG 只注入 top-k 相关片段
  - 工具调用结果先外部预处理/压缩，再喂模型
  - 实现“滚动窗口”：只保留最近 N 轮或 N tokens
- 测试多轮场景：
  - 模拟 10-20 轮对话，观察延迟、token 消耗、是否报错
  - 目标：平均会话 < 8k-12k tokens，峰值 < 模型窗口的 70-80%
- 利用厂商特性：
  - Prompt caching（Claude、Gemini 支持）：重复内容几乎免费
  - System prompt 固定不变部分，减少重复输入

### 后期阶段：上线监控与持续优化
- 设置 token / 轮次上限，超限时提示用户“开启新会话”或“精简问题”
- 监控生产数据：
  - 平均/峰值 token 用量
  - 长上下文错误率
  - 多轮对话掉线或成本异常
- 定期切换更优 API：关注新版本模型的 KV 优化（2026 年 GQA/MQA 已成主流）
- 成本敏感时：用缓存机制或多模型路由（短对话用便宜模型，长轨迹用大窗口模型）

一句话总结  
**KV Cache 是多轮对话和长 Agent 的“隐形杀手”，API 开发的核心工程实践是：主动压缩上下文 + 优先 GQA/MQA 模型 + 善用缓存机制。**