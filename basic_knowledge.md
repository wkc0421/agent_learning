# LLM应用开发关键知识点总结

## 1. Decoder-only vs Encoder-only vs Encoder-Decoder

### 核心区别与适用场景

- **Decoder-only**（代表模型：GPT系列、LLaMA、Mistral、Qwen、DeepSeek等主流开源/闭源模型）
  - 特点：自回归（autoregressive）生成，单向从左到右依次预测下一个token
  - 优势：生成能力极强，适合开放式、交互式任务
  - 典型应用：聊天机器人、文本续写、代码生成、Agent工具调用、建议生成、创意写作
  - 当前趋势：2025-2026年95%+的生成类应用（包括RAG、Agent）都使用Decoder-only架构
  - 适用阶段：几乎所有以“生成”为核心的LLM应用

- **Encoder-only**（代表模型：BERT、RoBERTa、DeBERTa等）
  - 特点：双向编码，整句上下文同时理解
  - 优势：对上下文的理解精度最高，适合“理解”而非“生成”任务
  - 典型应用：文本分类、情感分析、命名实体识别（NER）、语义相似度、embedding提取（RAG检索阶段常用）
  - 适用阶段：需要高质量表示或分类的子模块（常作为辅助模型）

- **Encoder-Decoder**（代表模型：T5、Flan-T5、BART、mT5）
  - 特点：编码器双向理解输入，解码器自回归生成输出，经典seq2seq架构
  - 优势：结构化转换任务表现稳定
  - 典型应用：机器翻译、文本摘要、文本改写、问答（输入-输出明确对应）
  - 适用阶段：输入输出有明确序列转换关系的任务

### 应用开发中的选择建议
- **生成主导的应用**（聊天、Agent、建议、代码、创作） → 优先Decoder-only（几乎是默认选择）
- **纯理解/分类/embedding** → Encoder-only
- **严格的序列到序列转换** → Encoder-Decoder（或用Decoder-only + 强prompt模拟）
- **错误选择后果**：选错架构会导致RAG/Agent效果明显下降、生成不稳定或效率低下

## 2. Self-Attention 的核心代价：O(n²) 时间 & 空间复杂度

### 核心概念
- n = 序列长度（context length，即输入 + 历史对话 + 检索内容等的总token数）
- Self-Attention机制需要对序列中每个token与其他所有token计算注意力 → 时间和空间复杂度均为 **O(n²)**
- 影响：
  - 上下文越长，推理速度越慢
  - 显存/内存占用呈平方级增长
  - 云API按token计费时，成本随n快速上升（接近指数级）

### 实际应用中的表现
- 长上下文（128k、200k、1M、2M）非常贵、慢、吃资源
- 典型问题：
  - 推理延迟显著增加
  - 云API单次查询成本大幅上升
  - 本地部署时显存容易溢出

### 开发中的应对策略（API调用场景为主）

#### 早期阶段（需求分析 & 模型选型）
- 估算典型和最坏情况下的上下文长度（n）
- 优先选择支持长上下文且性价比高的API（如Grok、Gemini、Claude的长版本）
- 关注是否支持prompt caching（重复内容缓存可降成本90%+）
- 粗算成本模型：成本 ≈ (输入tokens × 单价 + 输出tokens × 单价) × 频次

#### 中期阶段（原型 & 优化）
- 主动压缩上下文：
  - RAG只注入top-k相关chunk
  - 历史对话用摘要代替全量
  - 工具返回的长数据先总结/提取关键指标
- 测试不同n下的延迟和成本，设定目标（如平均<5k-8k tokens，延迟<3-5s）
- 使用多模型路由：简单查询用短上下文便宜模型，复杂长分析用长上下文模型

#### 后期阶段（上线 & 运维）
- 实时token限流（超阈值提示用户精简或新会话）
- 监控长上下文查询占比，持续优化
- 利用厂商的context caching机制
- 定期复盘API价格与性能，选择最优提供商

### 总结一句话
**Decoder-only 是当下生成类应用的绝对主流，但Self-Attention的O(n²)代价决定了“控制有效上下文长度”是API调用开发中最重要的工程实践之一。**